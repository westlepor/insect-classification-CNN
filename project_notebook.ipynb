{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bee Image Classification using a CNN to detect Varroa mite infestation\n",
    "\n",
    "#### Author: Mahdi Shadkam-Farrokhi: [GitHub](https://github.com/Shaddyjr) | [Medium](https://medium.com/@mahdis.pw) | [LinkedIn](https://www.linkedin.com/in/mahdi-shadkam-farrokhi-m-s-8a410958/) | [mahdis.pw](http://mahdis.pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Of the [many likely causes](https://www.sciencenewsforstudents.org/article/why-are-bees-vanishing-pesticides-disease-other-threats) behind the drastic decline in the honey bee population, the _Varroa (pronounced \"vr-ow-uh\") destructor_, or more commonly the _varroa mite_, is among the top contributors. As a natural predator to honey bees, the varroa mite is one of the biggest pests plaguing the bee keeping community. \n",
    "\n",
    "\n",
    "While small, these mites are difficult for bees to deal with. However, bee keepers can employ a number of treatments to help rid a hive of a varroa infestation. The sooner a bee keeper can begin treatment, the less likely a [colony collapse](https://ipm.missouri.edu/MPG/2013/7/Colony-Collapse-Disorder-the-Varroa-Mite-and-Resources-for-Beekeepers/) will occur.\n",
    "\n",
    "Having an tool to quickly assess a colony's health could mean the difference between a healthy hive and a dead one. I plan to create that tool using the dataset from the [Honey Bee Annotated Image Dataset](https://www.kaggle.com/jenny18/honey-bee-annotated-images) found on Kaggle.\n",
    "\n",
    "Using this dataset, I will use a Convolutional Neural Network to create a predictive model to determine if a bee image shows evidence of varroa mites or not (binary classification).\n",
    "\n",
    "A bee hive with a varroa mite infestation can quickly end up dead from varroa collapse. As such, falsely classifying an image as clear could be disastrous. Therefore, recall, or sensativity, will be the metric I use for model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Data Dictionary](#Data-Dictionary)\n",
    "- [Imports and loading Data](#Imports-and-loading-Data)\n",
    "- [Preliminary EDA](#Preliminary-EDA)\n",
    "    - [Handling target variable](#Handling-target-variable)\n",
    "    - [Clearing unnecessary columns](#Clearing-unnecessary-columns)\n",
    "    - [Visualizing file images](#Visualizing-file-images)\n",
    "- [Source Documentation](#Source-Documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "The data used for this analysis was taken directly from the [Honey Bee Annotated Image Dataset](https://www.kaggle.com/jenny18/honey-bee-annotated-images) fround on Kaggle.\n",
    "\n",
    "The following is the data dictionary for the original dataset:\n",
    "\n",
    "|column|type|description|\n",
    "|-|-|-|\n",
    "|file|string|name of file in 'bee_imgs' folder|\n",
    "|date|string|date image was captured|\n",
    "|time|string|time of day of image capture (military time)|\n",
    "|location|string|Location (city, state, country)|\n",
    "|zip|int| Zip code of the location|\n",
    "|subspecies|string|Subspecies of _Apis mellifera_ species|\n",
    "|health|string|Health of the bee|\n",
    "|pollen_carrying|boolean|Presence of pollen on the bee's legs|\n",
    "|caste|string|Worker, Drone, or Queen bee|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from skimage.transform import rescale, resize, rotate\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/bee_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_counts = data[\"health\"].value_counts()\n",
    "\n",
    "plt.title(\"Counts of bee $health$ categories\")\n",
    "sns.barplot(x = health_counts, y = health_counts.index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `healthy` and varroa-associated categories are the observations of interest and comprise the majority of total observations in the dataset. \n",
    "\n",
    "_The other observations will be dropped_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categories = [name for name in health_counts.index if \"varr\" in name.lower() or \"health\" in name.lower()]\n",
    "data = data[[status in target_categories for status in data[\"health\"]]]\n",
    "data[\"has_varroa\"] = (data[\"health\"] != \"healthy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_counts = data[\"has_varroa\"].value_counts()\n",
    "\n",
    "plt.title(\"Counts of bee $health$ categories\")\n",
    "sns.barplot(x = health_counts, y = health_counts.index, orient=\"h\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"has_varroa\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data are somewhat unbalanced, with the target variable `has_varroa` accounting for almost 24% of the data having the observation of interest.\n",
    "\n",
    "With 76% of the dataset consisting of healthy bees, this serves as the baseline model's accuracy. Any model with an accuracy significantly higher than 76% can be considered superior to this naive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"location\").mean()[[\"has_varroa\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently only two locations actually showcased varroa, which could train the model on factors unrelated to infected bees, such as classifying aspects of the location (parts of the image background) and/or the photographer's style (angle of shot, orientation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing unnecessary columns\n",
    "While a typical model would benefit from the image annotations, I'm attempting to make practical classifications based solely on a user's image. As such, most of the columns in the dataset are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"file\",\"has_varroa\"]]\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualizing file images\n",
    "Viewing the actual images that will be used to train the CNN may help inform how best to approach potentially cleaning and/or preparing the images for classification training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(files):\n",
    "    IMAGE_FILE_ROOT = './data/bee_imgs/' \n",
    "    return np.asanyarray([imageio.imread(\"{}{}\".format(IMAGE_FILE_ROOT, file)) for file in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_images(data[\"file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, ax = plt, title = None, show_size = False):\n",
    "    ax.imshow(image)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if not show_size:\n",
    "        ax.tick_params(bottom = False, left = False, labelbottom = False, labelleft = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(files, titles = None):\n",
    "    cols = 4\n",
    "    f, ax = plt.subplots(nrows=int(np.ceil(len(files)/cols)),ncols=cols, figsize=(14,8))\n",
    "    ax = ax.flatten()\n",
    "    for i, file in enumerate(files):\n",
    "        if titles:\n",
    "            show_image(file, ax = ax[i], title = titles[i])\n",
    "        else:\n",
    "            show_image(file, ax = ax[i], title = None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(images[::1200],list(data[\"has_varroa\"].map({1:\"has_varroa\",0:\"healthy\"})[::1200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These pixelated images will be digested and processed by the CNN to descern the bee's classfication. \n",
    "\n",
    "However, from quickly viewing just a handful of these images, I'm concerned about the low resolution and inconsistent quality of the data. For example, the 2nd image above shows a bee with varroa, which appears as little more than a blob. \n",
    "\n",
    "Can a CNN accurately discern the difference between one blob and another? If it can, is it really learning to distinguish an infected bee from a healthy one, or is it learning to distinguish something else, like the background?\n",
    "\n",
    "Additionally, I'm concerned the inconsistent image sizes may pose a problem when inputting the pixel data into the convolutional layer. These images will need to be standardized, and using either resizing or rescaling will either lose data or distort the image (effectively using augmented data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering most frequent image shape\n",
    "Since the CNN will need a consistent format, I'll use the most frequent image size by default and transform any images not matching those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_sizes(images):\n",
    "    out = {}\n",
    "    for image in images:\n",
    "        h, w, rbg = image.shape\n",
    "        size = \"{}x{}\".format(h,w)\n",
    "        if not out.get(size):\n",
    "            out[size] = 0\n",
    "        out[size] += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = get_image_sizes(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(sizes.values()) / sum(sizes.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only about .3% (12 total) of the total image data has the most frequent size, we'll have to rethink this approach. Instead, perhaps an average of the widths and heights can be used to coerce images to the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_wh(images):\n",
    "    widths = []\n",
    "    heights = []\n",
    "    for image in images:\n",
    "        h, w, rbg = image.shape\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "    return (widths, heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_average(dist, cutoff = .5):\n",
    "    # requires single peak normal-like distribution\n",
    "    hist, bin_edges = np.histogram(dist, bins = 25);\n",
    "    total_hist = sum(hist)\n",
    "    hist_edges = [(vals[0]/total_hist,vals[1]) for vals in zip(hist, bin_edges)]\n",
    "    hist_edges.sort(key = lambda x: x[0])\n",
    "    lefts = []\n",
    "    while cutoff > 0:\n",
    "        vals = hist_edges.pop()\n",
    "        cutoff -= vals[0]\n",
    "        lefts.append(vals[1])\n",
    "    diff = np.abs(np.diff(lefts)[0]) # same diff b/c of bins\n",
    "    leftmost = min(lefts)\n",
    "    rightmost = max(lefts) + diff\n",
    "    return int(np.round(np.mean([rightmost,leftmost])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = get_images_wh(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Widths\")\n",
    "plt.hist(wh[0], bins = 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Heights\")\n",
    "plt.hist(wh[1], bins = 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDEAL_WIDTH, IDEAL_HEIGHT = get_best_average(wh[0]), get_best_average(wh[1])\n",
    "IDEAL_WIDTH, IDEAL_HEIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These average widths will do the least to deform/alter the original image for the majority of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageHandler Class\n",
    "\n",
    "[found here](./ImageHandler_doc.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_handler import ImageHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gridSearchCNN helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearchCNN():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing images\n",
    "With an ideal height and width set, we can create a simple function responsible for handling the resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, resizing = (28,28), preserve_range=True):\n",
    "    return resize(image, resizing, anti_aliasing=True, mode = \"constant\", preserve_range = preserve_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing = (IDEAL_WIDTH, IDEAL_HEIGHT, 3)\n",
    "f, axes = plt.subplots(ncols = 4, figsize = (10,14))\n",
    "\n",
    "old_image_1 = imageio.imread(\"./data/bee_imgs/017_100.png\")\n",
    "show_image(old_image_1, ax = axes[0], title = \"Original Image #1\", show_size = True)\n",
    "new_image_1 = resize_image(old_image_1, resizing)\n",
    "show_image(new_image_1, ax = axes[1], title = \"Resized to standardized size\", show_size = True)\n",
    "\n",
    "old_image_2 = imageio.imread(\"./data/bee_imgs/017_010.png\")\n",
    "show_image(old_image_2, ax = axes[2], title = \"Original Image #2\", show_size = True)\n",
    "new_image_2 = resize_image(old_image_2, resizing)\n",
    "show_image(new_image_2, ax = axes[3], title = \"Resized to standardized size\", show_size = True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, the images are not terribly perturbed by resizing them to a standard width and height. Of course, these augmented images must be taken into consideration when interpreting the results and the applicability of the model to real world images, which may require a more nuanced method for standardizing incoming images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "We may want to consider boosting (bootstrapping) the minority class (has_varroa), given unbalanced classes.\n",
    "\n",
    "Found some advice for working with CNN - [source](https://victorzhou.com/blog/keras-cnn-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(files, resizing):\n",
    "    '''Normalizes ...............'''\n",
    "    IMAGE_FILE_ROOT = './data/bee_imgs/' \n",
    "    return (np.asanyarray([resize_image(imageio.imread(\"{}{}\".format(IMAGE_FILE_ROOT, file)), resizing) for file in files]) / 255) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing = (IDEAL_WIDTH, IDEAL_HEIGHT, 3) # needed to remove 4th alpha channel\n",
    "\n",
    "X = get_image_data(data[\"file\"], resizing)\n",
    "\n",
    "y = data[\"has_varroa\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, stratify = y, random_state = random_state)\n",
    "\n",
    "y_train_dummy = np_utils.to_categorical(y_train,2)\n",
    "y_test_dummy  = np_utils.to_categorical(y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_filters = 16\n",
    "density_units = 32\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 16\n",
    "\n",
    "kernel_size = 3\n",
    "dropout = .5\n",
    "activation_func = \"relu\"\n",
    "\n",
    "# Instantiate a CNN.\n",
    "cnn_model_2 = Sequential()\n",
    "\n",
    "# Add a convolutional layer.\n",
    "cnn_model_2.add(Conv2D(filters   = initial_filters,         # number of filters\n",
    "                     kernel_size = kernel_size,        # height/width of filter\n",
    "                     activation  = activation_func,      # activation function \n",
    "                     input_shape = (IDEAL_WIDTH,IDEAL_HEIGHT,3)) # shape of input (image)\n",
    "               )\n",
    "\n",
    "# Add a pooling layer.\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2))) # dimensions of region of pooling\n",
    "\n",
    "# Add another convolutional layer.\n",
    "cnn_model_2.add(Conv2D(filters = initial_filters * 2,\n",
    "                       kernel_size = kernel_size,\n",
    "                       activation=activation_func))\n",
    "\n",
    "# Add another pooling layer.\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We have to remember to flatten to go from the \"box\" to the vertical line of nodes!\n",
    "cnn_model_2.add(Flatten())\n",
    "\n",
    "# Add a densely-connected layer with 64 neurons.\n",
    "cnn_model_2.add(Dense(density_units * 2, activation=activation_func))\n",
    "\n",
    "# Let's try to avoid overfitting!\n",
    "cnn_model_2.add(Dropout(dropout))\n",
    "\n",
    "# Add a densely-connected layer with 32 neurons.\n",
    "cnn_model_2.add(Dense(density_units, activation=activation_func))\n",
    "\n",
    "# Let's try to avoid overfitting!\n",
    "cnn_model_2.add(Dropout(dropout))\n",
    "\n",
    "# Add a final layer with 2 neurons.\n",
    "cnn_model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "cnn_model_2.compile(loss='binary_crossentropy',\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"binary_accuracy\"])\n",
    "\n",
    "# Fit model on training data\n",
    "history = cnn_model_2.fit(X_train,\n",
    "                          y_train_dummy,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(X_test, y_test_dummy),\n",
    "                          epochs=epochs,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
    "plt.plot(test_loss, label='Testing Loss', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(len(test_loss)), labels = range(1,len(test_loss) + 1))\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn_model_2.predict_classes(X_test)\n",
    "\n",
    "results = predictions - y_test\n",
    "results.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see 1.11% of the test predictions were false positives, while .8% were false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix_stats(y_test, preds):\n",
    "    ''' Return key confusion matrix metrics given true and predicted values'''\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    TP, FP, FN, TN, = cm[1,1], cm[0,1], cm[1,0], cm[0,0]\n",
    "    total = (TP + FP + FN + TN)\n",
    "    acc = (TP + TN ) / total\n",
    "    miss = 1 - acc\n",
    "    sens = TP / (TP + FN)\n",
    "    spec = TN / (TN + FP)\n",
    "    prec = TP / (TP + FP)\n",
    "    return {\"accuracy\": acc, \"miss_rate\": miss, \"sensitivity\": sens, \"specification\": spec, \"precision\": prec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_stats(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model's __accuracy is 99.2%__ with a specificity of 99.97% and a sensativity of 94.6%.\n",
    "\n",
    "As this model performs much better than the baseline model, we can conclude this model is a more useful classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bad_image(file, ax, title = None, IMAGE_FILE_ROOT = './data/bee_imgs/'):\n",
    "    image = imageio.imread(\"{}{}\".format(IMAGE_FILE_ROOT,file)) if type(file) == str else file\n",
    "    shape = image.shape\n",
    "    if title:\n",
    "        ax.set_title(f\"Orig. size: {shape[0]}x{shape[1]}\\n{title}\")\n",
    "    ax.imshow(resize_image(image, resizing, preserve_range = False))\n",
    "    ax.tick_params(bottom = False, left = False, labelbottom = False, labelleft = False)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bad_images(files, titles = None, ncols = 4, height = 4):\n",
    "    nrows = int(np.ceil(len(files)/ncols))\n",
    "    f, ax = plt.subplots(nrows=nrows,ncols=ncols, figsize=(10,nrows * height))\n",
    "    ax = ax.flatten()\n",
    "    for i, file in enumerate(files):\n",
    "            show_bad_image(file, ax = ax[i], title = titles[i] if titles else titles)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_files = data.loc[results[results == -1].index][\"file\"]\n",
    "show_bad_images(false_positive_files, [\"false pos.\"] * len(false_positive_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see these false positives have some common issues that are likely throwing off the neural network.\n",
    "\n",
    "1. __Rescaling artifacts__ - the rescaling step may have drastically altered the original image\n",
    "2. __Orientation__ - many of the bees pictured here are upside-down. This may be remedied by retraining the network using rotated versions of the training data.\n",
    "3. __Patterned background__ - a striped pattern is a signature part of the bee abdomen. Some of these falsely classified images could have been the result of the network mistaking the background for a larger bee with a \"smaller\" mite on it.\n",
    "4. __Shadows__ - a number of these images display prominent shadows, which may have confused the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_files = data.loc[results[results == 1].index][\"file\"]\n",
    "show_bad_images(false_negative_files, [\"false neg.\"] * len(false_negative_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see these false negatives show some common issues as well:\n",
    "\n",
    "1. __Shadows__ - like before, a number of these images display prominent shadows, which may have confused the network.\n",
    "2. __Partials__ - many of these images show either more than 1 bee or only parts of bees. This could easily confuse the network, as these kinds of images are essentially incomplete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining with rotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotated_image_data(files, resizing, preserve_range = True, normalize = True):\n",
    "    IMAGE_FILE_ROOT = './data/bee_imgs/'\n",
    "    out = []\n",
    "    for file in files:\n",
    "        merger = []\n",
    "        img = resize_image(imageio.imread(\"{}{}\".format(IMAGE_FILE_ROOT, file)), resizing, preserve_range=preserve_range)\n",
    "        if normalize:\n",
    "            img = (img / 255) - .5\n",
    "        merger.append(img)\n",
    "        merger.extend([rotate(img, angle) for angle in range(90,360,90)])\n",
    "        merger.extend([np.flipud(unflipped_img) for unflipped_img in merger])\n",
    "        out.extend(merger)\n",
    "    return np.asanyarray(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bad_images(get_rotated_image_data(data[\"file\"].iloc[0:1], resizing, preserve_range = False, normalize = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is rotated in all 4 direction and mirrored. This ensures the model is trained on as many possible orientations as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = get_rotated_image_data(data[\"file\"], resizing)\n",
    "new_y = np.repeat(y,8) # each image is replicated 8 times, essentially\n",
    "\n",
    "X_train_rotated, X_test_rotated, y_train, y_test = train_test_split(new_X, new_y, test_size = .25, stratify = new_y, random_state = random_state)\n",
    "\n",
    "y_train_dummy = np_utils.to_categorical(y_train,2)\n",
    "y_test_dummy  = np_utils.to_categorical(y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_filters = 16\n",
    "density_units = 32\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 16\n",
    "\n",
    "kernel_size = 3\n",
    "dropout = .5\n",
    "activation_func = \"relu\"\n",
    "\n",
    "# Instantiate a CNN.\n",
    "cnn_model_2 = Sequential()\n",
    "\n",
    "# Add a convolutional layer.\n",
    "cnn_model_2.add(Conv2D(filters   = initial_filters,         # number of filters\n",
    "                     kernel_size = kernel_size,        # height/width of filter\n",
    "                     activation  = activation_func,      # activation function \n",
    "                     input_shape = (IDEAL_WIDTH,IDEAL_HEIGHT,3)) # shape of input (image)\n",
    "               )\n",
    "\n",
    "# Add a pooling layer.\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2))) # dimensions of region of pooling\n",
    "\n",
    "# Add another convolutional layer.\n",
    "cnn_model_2.add(Conv2D(filters = initial_filters * 2,\n",
    "                       kernel_size = kernel_size,\n",
    "                       activation=activation_func))\n",
    "\n",
    "# Add another pooling layer.\n",
    "cnn_model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# We have to remember to flatten to go from the \"box\" to the vertical line of nodes!\n",
    "cnn_model_2.add(Flatten())\n",
    "\n",
    "# Add a densely-connected layer with 64 neurons.\n",
    "cnn_model_2.add(Dense(density_units * 2, activation=activation_func))\n",
    "\n",
    "# Let's try to avoid overfitting!\n",
    "cnn_model_2.add(Dropout(dropout))\n",
    "\n",
    "# Add a densely-connected layer with 32 neurons.\n",
    "cnn_model_2.add(Dense(density_units, activation=activation_func))\n",
    "\n",
    "# Let's try to avoid overfitting!\n",
    "cnn_model_2.add(Dropout(dropout))\n",
    "\n",
    "# Add a final layer with 2 neurons.\n",
    "cnn_model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "cnn_model_2.compile(loss='binary_crossentropy',\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"binary_accuracy\"])\n",
    "\n",
    "# Fit model on training data\n",
    "history = cnn_model_2.fit(X_train_rotated,\n",
    "                          y_train_dummy,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(X_test_rotated, y_test_dummy),\n",
    "                          epochs=epochs,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
    "plt.plot(test_loss, label='Testing Loss', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(len(test_loss)), labels = range(1,len(test_loss) + 1))\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = cnn_model_2.predict_classes(X_test)\n",
    "results2 = predictions2 - y_test\n",
    "results2.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_stats(y_test, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_files = data.loc[results2[results2 == -1].index][\"file\"].unique()\n",
    "show_bad_images(false_positive_files, [\"false pos.\"] * len(false_positive_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many images are completely lacking in definition\n",
    "- Another noteworthy trend is the darker exoskeletons of certain species of bees, which make them more prone to being falsely classified. This model only takes the images as they are and does not take any additional data into account. If a bee \"species\" could be included, it would allow the model to more accurately distinguish darker bees with varroa mites.\n",
    "- confusing background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_files = data.loc[results2[results2 == 1].index][\"file\"].unique()\n",
    "show_bad_images(false_negative_files, [\"false neg.\"] * len(false_negative_files), ncols = 6, height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_2.save_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "# num_filters = 8\n",
    "# filter_size = 3\n",
    "# pool_size = 2\n",
    "\n",
    "# # Build the model.\n",
    "# model = Sequential([\n",
    "#   Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "#   MaxPooling2D(pool_size=pool_size),\n",
    "#   Flatten(),\n",
    "#   Dense(10, activation='softmax'),\n",
    "# ])\n",
    "\n",
    "# # Load the model's saved weights.\n",
    "# model.load_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing color!\n",
    "Removing color significantly lowers accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_invert_images(images, add_dim = True):\n",
    "    originals = np.asanyarray([rgb2gray(img) for img in images])\n",
    "    inverted = 255 - originals\n",
    "    out = np.concatenate([originals, inverted])\n",
    "    \n",
    "    if add_dim:\n",
    "        return np.expand_dims(out, axis=3)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gray_inv = grayscale_invert_images(new_X)\n",
    "y_gray_inv = np.concatenate([new_y, new_y]) # each image is replicated 2 times, essentially\n",
    "\n",
    "X_train_gray_inv, X_test_gray_inv, y_train, y_test = train_test_split(X_gray_inv, y_gray_inv, \n",
    "                                                                      test_size = .25, stratify = y_gray_inv, \n",
    "                                                                      random_state = random_state)\n",
    "y_train_gray_inv_dummy = np_utils.to_categorical(y_train,2)\n",
    "y_test_gray_inv_dummy  = np_utils.to_categorical(y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(X_gray_inv[0]), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(X_gray_inv[X_gray_inv.shape[0] // 2]), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_filters = 16\n",
    "density_units = 32\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 8\n",
    "\n",
    "kernel_size = 3\n",
    "dropout = .5\n",
    "activation_func = \"relu\"\n",
    "\n",
    "# Instantiate a CNN.\n",
    "cnn_model_2 = Sequential([\n",
    "    Conv2D(filters = initial_filters, kernel_size = kernel_size, \n",
    "           activation  = activation_func, input_shape = (IDEAL_WIDTH,IDEAL_HEIGHT, 1)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(filters = initial_filters * 2, kernel_size = kernel_size, activation=activation_func),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(density_units * 2, activation=activation_func),\n",
    "    Dropout(dropout),\n",
    "    Dense(density_units, activation=activation_func),\n",
    "    Dropout(dropout),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "cnn_model_2.compile(loss='binary_crossentropy',\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"binary_accuracy\"])\n",
    "\n",
    "# Fit model on training data\n",
    "history = cnn_model_2.fit(X_train_gray_inv,\n",
    "                          y_train_gray_inv_dummy,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(X_test_gray_inv, y_test_gray_inv_dummy),\n",
    "                          epochs=epochs,\n",
    "                          verbose=1)\n",
    "\n",
    "# # Load the model's saved weights.\n",
    "# cnn_model_2.load_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = cnn_model_2.predict_classes(X_test)\n",
    "results2 = predictions2 - y_test\n",
    "results2.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_files = data.loc[results2[results2 == -1].index][\"file\"].unique()\n",
    "show_bad_images(false_positive_files, [\"false pos.\"] * len(false_positive_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_files = data.loc[results2[results2 == 1].index][\"file\"].unique()\n",
    "show_bad_images(false_negative_files, [\"false neg.\"] * len(false_negative_files), ncols = 6, height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 3 false negatives, this mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just inverting image\n",
    "\n",
    "97.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_images(images, add_dim = True):\n",
    "    return 1 - images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_image_data(files, resizing):\n",
    "    '''Normalizes ...............'''\n",
    "    IMAGE_FILE_ROOT = './data/bee_imgs/' \n",
    "    return (np.asanyarray([resize_image(imageio.imread(\"{}{}\".format(IMAGE_FILE_ROOT, file)), resizing, preserve_range=False) for file in files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_X = invert_images(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resizing = (IDEAL_WIDTH, IDEAL_HEIGHT, 3) # needed to remove 4th alpha channel\n",
    "\n",
    "X = _get_image_data(data[\"file\"], resizing)\n",
    "i_X = invert_images(X)\n",
    "X = np.concatenate([X,i_X])\n",
    "\n",
    "y = np.concatenate([data[\"has_varroa\"], data[\"has_varroa\"]])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, stratify = y, random_state = random_state)\n",
    "\n",
    "y_train_dummy = np_utils.to_categorical(y_train,2)\n",
    "y_test_dummy  = np_utils.to_categorical(y_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_filters = 16\n",
    "density_units = 32\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "kernel_size = 3\n",
    "dropout = .5\n",
    "activation_func = \"relu\"\n",
    "\n",
    "# Instantiate a CNN.\n",
    "cnn_model_2 = Sequential([\n",
    "    Conv2D(filters = initial_filters, kernel_size = kernel_size, \n",
    "           activation  = activation_func, input_shape = (IDEAL_WIDTH,IDEAL_HEIGHT, 3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(filters = initial_filters * 2, kernel_size = kernel_size, activation=activation_func),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(density_units * 2, activation=activation_func),\n",
    "    Dropout(dropout),\n",
    "    Dense(density_units, activation=activation_func),\n",
    "    Dropout(dropout),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "cnn_model_2.compile(loss='binary_crossentropy',\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"binary_accuracy\"])\n",
    "\n",
    "# Fit model on training data\n",
    "history = cnn_model_2.fit(X_train,\n",
    "                          y_train_dummy,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=(X_test, y_test_dummy),\n",
    "                          epochs=epochs,\n",
    "                          verbose=1)\n",
    "\n",
    "# # Load the model's saved weights.\n",
    "# cnn_model_2.load_weights('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='#185fad')\n",
    "plt.plot(test_loss, label='Testing Loss', color='orange')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(len(test_loss)), labels = range(1,len(test_loss) + 1))\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = cnn_model_2.predict_classes(X_test)\n",
    "results2 = pd.Series(predictions2 - y_test)\n",
    "results2.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_files = data.loc[results2[results2 == -1].index][\"file\"].unique()\n",
    "show_bad_images(false_positive_files, [\"false pos.\"] * len(false_positive_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the CNN\n",
    "- https://www.analyticsvidhya.com/blog/2019/05/understanding-visualizing-neural-networks/\n",
    "- https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/\n",
    "- https://datascience.stackexchange.com/questions/14899/how-to-draw-deep-learning-network-architecture-diagrams\n",
    "- https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/\n",
    "- http://alexlenail.me/NN-SVG/LeNet.html\n",
    "- https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/3\n",
    "- https://datascience.stackexchange.com/questions/14899/how-to-draw-deep-learning-network-architecture-diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negative_files = data.loc[results2[results2 == 1].index][\"file\"].unique()\n",
    "show_bad_images(false_negative_files, [\"false neg.\"] * len(false_negative_files), ncols = 6, height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageHandler class\n",
    "\n",
    "Was very inconvenient working with image data, especially when wanting to transform the data for both modeling and visualization.\n",
    "\n",
    "I created the ImageHandler class simply to alleviate the hassle of keeping track of image data, while also allowing for intuitive transformations in a consolidated way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(files):\n",
    "    IMAGE_FILE_ROOT = './data/bee_imgs/' \n",
    "    return [imageio.imread(\"{}{}\".format(IMAGE_FILE_ROOT, file)) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = get_image_data(data[\"file\"].values)\n",
    "\n",
    "from image_handler import ImageHandler\n",
    "\n",
    "img_store = ImageHandler(raw_images)\n",
    "\n",
    "ImageHandler._images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO \n",
    "- Organize code w/OOP or functional design\n",
    "- ~~try grayscale w/inverted colors (no good)~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "## Overview\n",
    "\n",
    "Progress report due Monday August 12, 2019.\n",
    "\n",
    "The finish line is in sight! You should have your data in hand and some models made. This portion of the Capstone contains two parts to help really hone your model and develop mastery over your final project.\n",
    "\n",
    "*Goal*: A written progress report on your project and a detailed 1:1 with your instructor.\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The actual deliverable for this section is fairly small. You are expected to submit, via [this Google form](https://forms.gle/ci7maWp4G5wRHdwH8), a progress report detailing where you are with your Capstone. The format and content is up to you but, at a minimum, we expect the following:\n",
    "\n",
    "1. Do you have data fully in hand and if not, what blockers are you facing?\n",
    "2. Have you done a full EDA on all of your data?\n",
    "3. Have you begun the modeling process? How accurate are your predictions so far?\n",
    "4. What blockers are you facing, including processing power, data acquisition, modeling difficulties, data cleaning, etc.? How can we help you overcome those challenges?\n",
    "5. Have you changed topics since your lightning talk? Since you submitted your Problem Statement and EDA? If so, do you have the necessary data in hand (and the requisite EDA completed) to continue moving forward?\n",
    "6. What is your timeline for the next week and a half? What do you _have_ to get done versus what would you _like_ to get done?\n",
    "7. What topics do you want to discuss during your 1:1?\n",
    "\n",
    "We will use your progress report as a leaping off point for a deep 1:1 with a member of the instructional staff. The best use of your time is to really flesh out this document so that your remaining time here can be most productive.\n",
    "\n",
    "Submit this document no later than *end of day, August 12, 2019*.\n",
    "\n",
    "## Necessary Deliverables / Submission\n",
    "\n",
    "- Your progress report (touching on the points above) submitted no later than *end of day, August 12, 2019* on [this form](https://forms.gle/ci7maWp4G5wRHdwH8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Documentation\n",
    "- [Honey Bee Annotated Image Dataset (Kaggle)](https://www.kaggle.com/jenny18/honey-bee-annotated-images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
